{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CS 3780/5780 Creative Project: </h2>\n",
    "<h3>Emotion Classification of Natural Language</h3>\n",
    "\n",
    "Names and NetIDs for your group members: James Tu (jt737), Andrew Cheung (aec295)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction:</h3>\n",
    "\n",
    "<p> The creative project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The past programming projects provide templates for how to do this (and you can reuse part of your code if you wish), and the lectures provide some of the methods you can use. So, this creative project brings realism to how you will use machine learning in the real world.  </p>\n",
    "\n",
    "The task you will work on is classifying texts to human emotions. Through words, humans express feelings, articulate thoughts, and communicate our deepest needs and desires. Language helps us interpret the nuances of joy, sadness, anger, and love, allowing us to connect with others on a deeper level. Are you able to train an ML model that recognizes the human emotions expressed in a piece of text? <b>Please read the project description PDF file carefully and follow the instructions there. Also make sure you write your code and answers to all the questions in this Jupyter Notebook </b> </p>\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 0: Basics</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.1 Import:</h3><p>\n",
    "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "    \n",
    "https://pytorch.org/tutorials/\n",
    "    \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.2 Accuracy and Mean Squared Error:</h3><p>\n",
    "To measure your performance in the Kaggle Competition, we are using accuracy. As a recap, accuracy is the percent of labels you predict correctly. To measure this, you can use library functions from sklearn. A simple example is shown below. \n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [3, 2, 1, 0, 1, 2, 3]\n",
    "y_true = [0, 1, 2, 3, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: Basic</h2><p>\n",
    "Note that your code should be commented well and in part 1.4 you can refer to your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1 Load and preprocess the dataset:</h3><p>\n",
    "We provide how to load the data on Kaggle's Notebook.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "# Split into text and label\n",
    "train_text = train[\"text\"]\n",
    "train_label = train[\"label\"]\n",
    "\n",
    "# Load the testing data\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "# Split into text and id\n",
    "test_id = test[\"id\"]\n",
    "test_text = test[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() # Show the first few rows of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head() # Show the first few rows of testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
    "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 Training, Validation and Model Selection:</h3><p>\n",
    "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we split our data into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_val = train_test_split(train_text, train_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we vectorize the training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english') # Remove stop words\n",
    "vectorizer_bow = CountVectorizer(stop_words='english') # Remove stop words\n",
    "\n",
    "# Use TF-IDF vectorizer for sentence embeddings\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "# Use bag of words vectorizer for sentence embeddings\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.1 Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/my_env/lib/python3.8/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "SVM Accuracy: 0.706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use grid search to find the best hyperparameters for SVM\n",
    "params = {\n",
    "  'C': [0.001, 0.01, 0.1, 1, 5],\n",
    "  'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), params, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_C, best_kernal = best_params['C'], best_params['kernel']\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel=best_kernal, C=best_C)\n",
    "svm_model.fit(X_train_tfidf, y_train) # Use TF-IDF embeddings for SVM\n",
    "\n",
    "# Predict and calculate accuracy for SVM model\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 Training the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/my_env/lib/python3.8/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.2\n",
      "Naive Bayes Accuracy: 0.5895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use grid search to find the best hyperparameters (alpha) for Naive Bayes\n",
    "params = {'alpha': [0.1 * x for x in range(1, 11)]} # Search alpha values 0.1 to 0.1\n",
    "grid_search = GridSearchCV(MultinomialNB(), params, scoring='accuracy')\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "best_alpha = grid_search.best_params_['alpha'] # Get the best alpha value\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "nb_model = MultinomialNB(alpha=best_alpha) # Use the best alpha value for Naive Bayes Model\n",
    "nb_model.fit(X_train_bow, y_train) # Use bag of words embeddings for Naive Bayes\n",
    "\n",
    "# Predict and calculate accuracy for Naive Bayes model\n",
    "y_pred_nb = nb_model.predict(X_test_bow)\n",
    "accuracy_nb = accuracy_score(y_val, y_pred_nb) \n",
    "print(f'Naive Bayes Accuracy: {accuracy_nb}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.3 Training the Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and calculate accuracy for XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "accuracy_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "print(f'XGBoost Accuracy: {accuracy_xgb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4 Explanation in Words:</h3><p>\n",
    "    You need to answer the following questions in the markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.1 How did you formulate the learning problem?\n",
    "\n",
    "1.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "1.4.3 How did you do the model selection?\n",
    "- SVM\n",
    "  - TFID represenetation due to weighting\n",
    "- Naive Bayes\n",
    "  - Bag of words representation due to probabilities\n",
    "  - Remove stop words\n",
    "  - tried n-grams (didn't work)\n",
    "  - Change alpha to 0.5 (GirdSearchCV)\n",
    "\n",
    "1.4.4 Does the test performance reach the first baseline \"Tiny Piney\"? (Please include a screenshot of Kaggle Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Be creative!</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Open-ended Code:</h3><p>\n",
    "You may follow the steps in part 1 again but making innovative changes like using new training algorithms, etc. Make sure you explain everything clearly in part 2.2. Note that beating \"Zero Hero\" is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 2.2\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(train_label)))\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmotionDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = EmotionDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=3e-5,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa317ecece545d1a08eaf38e7a399c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.421, 'grad_norm': 8.719630241394043, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.02}\n",
      "{'loss': 3.3764, 'grad_norm': 8.828363418579102, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 3.3023, 'grad_norm': 14.032581329345703, 'learning_rate': 1.8e-06, 'epoch': 0.06}\n",
      "{'loss': 3.1403, 'grad_norm': 10.317893028259277, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 3.0692, 'grad_norm': 7.361644744873047, 'learning_rate': 3e-06, 'epoch': 0.1}\n",
      "{'loss': 2.9781, 'grad_norm': 8.035880088806152, 'learning_rate': 3.6e-06, 'epoch': 0.12}\n",
      "{'loss': 3.0268, 'grad_norm': 10.154255867004395, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.14}\n",
      "{'loss': 2.9592, 'grad_norm': 7.992445468902588, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 2.9648, 'grad_norm': 7.501841068267822, 'learning_rate': 5.4e-06, 'epoch': 0.18}\n",
      "{'loss': 2.9135, 'grad_norm': 7.061291217803955, 'learning_rate': 6e-06, 'epoch': 0.2}\n",
      "{'loss': 2.813, 'grad_norm': 10.988604545593262, 'learning_rate': 6.6e-06, 'epoch': 0.22}\n",
      "{'loss': 2.7862, 'grad_norm': 5.423258304595947, 'learning_rate': 7.2e-06, 'epoch': 0.24}\n",
      "{'loss': 2.7056, 'grad_norm': 6.789228916168213, 'learning_rate': 7.8e-06, 'epoch': 0.26}\n",
      "{'loss': 2.5833, 'grad_norm': 9.70683479309082, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 2.5368, 'grad_norm': 7.675382137298584, 'learning_rate': 9e-06, 'epoch': 0.3}\n",
      "{'loss': 2.458, 'grad_norm': 7.191104412078857, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 2.2685, 'grad_norm': 6.096157073974609, 'learning_rate': 1.02e-05, 'epoch': 0.34}\n",
      "{'loss': 2.2567, 'grad_norm': 6.631343364715576, 'learning_rate': 1.08e-05, 'epoch': 0.36}\n",
      "{'loss': 2.0722, 'grad_norm': 6.709118843078613, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.38}\n",
      "{'loss': 2.1976, 'grad_norm': 6.885643482208252, 'learning_rate': 1.2e-05, 'epoch': 0.4}\n",
      "{'loss': 2.1308, 'grad_norm': 8.665999412536621, 'learning_rate': 1.26e-05, 'epoch': 0.42}\n",
      "{'loss': 2.1492, 'grad_norm': 6.62369441986084, 'learning_rate': 1.32e-05, 'epoch': 0.44}\n",
      "{'loss': 2.1045, 'grad_norm': 7.148601531982422, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.46}\n",
      "{'loss': 1.9279, 'grad_norm': 7.662140369415283, 'learning_rate': 1.44e-05, 'epoch': 0.48}\n",
      "{'loss': 1.8789, 'grad_norm': 8.264263153076172, 'learning_rate': 1.5e-05, 'epoch': 0.5}\n",
      "{'loss': 1.8551, 'grad_norm': 9.32882308959961, 'learning_rate': 1.56e-05, 'epoch': 0.52}\n",
      "{'loss': 1.8768, 'grad_norm': 6.88959264755249, 'learning_rate': 1.62e-05, 'epoch': 0.54}\n",
      "{'loss': 1.7922, 'grad_norm': 7.464666366577148, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 1.7207, 'grad_norm': 11.115447998046875, 'learning_rate': 1.74e-05, 'epoch': 0.58}\n",
      "{'loss': 1.7617, 'grad_norm': 17.8317928314209, 'learning_rate': 1.8e-05, 'epoch': 0.6}\n",
      "{'loss': 1.6556, 'grad_norm': 7.13623571395874, 'learning_rate': 1.86e-05, 'epoch': 0.62}\n",
      "{'loss': 1.6785, 'grad_norm': 10.683618545532227, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 1.811, 'grad_norm': 5.413241863250732, 'learning_rate': 1.98e-05, 'epoch': 0.66}\n",
      "{'loss': 1.6164, 'grad_norm': 4.962749004364014, 'learning_rate': 2.04e-05, 'epoch': 0.68}\n",
      "{'loss': 1.5833, 'grad_norm': 6.3516106605529785, 'learning_rate': 2.1e-05, 'epoch': 0.7}\n",
      "{'loss': 1.5668, 'grad_norm': 8.410833358764648, 'learning_rate': 2.16e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4871, 'grad_norm': 5.807083606719971, 'learning_rate': 2.22e-05, 'epoch': 0.74}\n",
      "{'loss': 1.5621, 'grad_norm': 14.086923599243164, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.76}\n",
      "{'loss': 1.6223, 'grad_norm': 5.835413932800293, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.78}\n",
      "{'loss': 1.4811, 'grad_norm': 11.8953218460083, 'learning_rate': 2.4e-05, 'epoch': 0.8}\n",
      "{'loss': 1.5064, 'grad_norm': 13.92353343963623, 'learning_rate': 2.4599999999999998e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2677, 'grad_norm': 7.119726181030273, 'learning_rate': 2.52e-05, 'epoch': 0.84}\n",
      "{'loss': 1.4553, 'grad_norm': 7.53139591217041, 'learning_rate': 2.58e-05, 'epoch': 0.86}\n",
      "{'loss': 1.6166, 'grad_norm': 9.433425903320312, 'learning_rate': 2.64e-05, 'epoch': 0.88}\n",
      "{'loss': 1.2875, 'grad_norm': 7.758480072021484, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.9}\n",
      "{'loss': 1.3152, 'grad_norm': 14.062267303466797, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 1.2113, 'grad_norm': 8.407808303833008, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.94}\n",
      "{'loss': 1.2476, 'grad_norm': 4.6932501792907715, 'learning_rate': 2.88e-05, 'epoch': 0.96}\n",
      "{'loss': 1.4003, 'grad_norm': 6.845582485198975, 'learning_rate': 2.94e-05, 'epoch': 0.98}\n",
      "{'loss': 1.1622, 'grad_norm': 11.957015991210938, 'learning_rate': 3e-05, 'epoch': 1.0}\n",
      "{'loss': 1.1253, 'grad_norm': 8.798605918884277, 'learning_rate': 2.985e-05, 'epoch': 1.02}\n",
      "{'loss': 1.1392, 'grad_norm': 7.84309720993042, 'learning_rate': 2.97e-05, 'epoch': 1.04}\n",
      "{'loss': 1.1074, 'grad_norm': 7.912227153778076, 'learning_rate': 2.955e-05, 'epoch': 1.06}\n",
      "{'loss': 1.0779, 'grad_norm': 8.327322006225586, 'learning_rate': 2.94e-05, 'epoch': 1.08}\n",
      "{'loss': 1.0456, 'grad_norm': 5.60709285736084, 'learning_rate': 2.925e-05, 'epoch': 1.1}\n",
      "{'loss': 1.0796, 'grad_norm': 8.740809440612793, 'learning_rate': 2.91e-05, 'epoch': 1.12}\n",
      "{'loss': 1.1438, 'grad_norm': 6.358974933624268, 'learning_rate': 2.895e-05, 'epoch': 1.14}\n",
      "{'loss': 1.0867, 'grad_norm': 10.856175422668457, 'learning_rate': 2.88e-05, 'epoch': 1.16}\n",
      "{'loss': 1.0662, 'grad_norm': 8.594893455505371, 'learning_rate': 2.865e-05, 'epoch': 1.18}\n",
      "{'loss': 1.0181, 'grad_norm': 7.599076747894287, 'learning_rate': 2.8499999999999998e-05, 'epoch': 1.2}\n",
      "{'loss': 1.0214, 'grad_norm': 8.437870025634766, 'learning_rate': 2.8349999999999998e-05, 'epoch': 1.22}\n",
      "{'loss': 1.0682, 'grad_norm': 17.970746994018555, 'learning_rate': 2.8199999999999998e-05, 'epoch': 1.24}\n",
      "{'loss': 0.869, 'grad_norm': 6.1695685386657715, 'learning_rate': 2.805e-05, 'epoch': 1.26}\n",
      "{'loss': 0.8523, 'grad_norm': 6.040217876434326, 'learning_rate': 2.79e-05, 'epoch': 1.28}\n",
      "{'loss': 0.9233, 'grad_norm': 6.570487022399902, 'learning_rate': 2.7750000000000004e-05, 'epoch': 1.3}\n",
      "{'loss': 0.8624, 'grad_norm': 54.987022399902344, 'learning_rate': 2.7600000000000003e-05, 'epoch': 1.32}\n",
      "{'loss': 0.9544, 'grad_norm': 4.1696014404296875, 'learning_rate': 2.7450000000000003e-05, 'epoch': 1.34}\n",
      "{'loss': 0.9435, 'grad_norm': 11.21460247039795, 'learning_rate': 2.7300000000000003e-05, 'epoch': 1.36}\n",
      "{'loss': 0.9128, 'grad_norm': 9.022870063781738, 'learning_rate': 2.7150000000000003e-05, 'epoch': 1.38}\n",
      "{'loss': 0.8153, 'grad_norm': 7.473583221435547, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.4}\n",
      "{'loss': 0.9077, 'grad_norm': 10.794825553894043, 'learning_rate': 2.6850000000000002e-05, 'epoch': 1.42}\n",
      "{'loss': 0.8124, 'grad_norm': 14.870903968811035, 'learning_rate': 2.6700000000000002e-05, 'epoch': 1.44}\n",
      "{'loss': 0.8692, 'grad_norm': 10.725987434387207, 'learning_rate': 2.655e-05, 'epoch': 1.46}\n",
      "{'loss': 0.9813, 'grad_norm': 13.315987586975098, 'learning_rate': 2.64e-05, 'epoch': 1.48}\n",
      "{'loss': 1.0744, 'grad_norm': 11.679426193237305, 'learning_rate': 2.625e-05, 'epoch': 1.5}\n",
      "{'loss': 0.7576, 'grad_norm': 8.233120918273926, 'learning_rate': 2.61e-05, 'epoch': 1.52}\n",
      "{'loss': 0.9506, 'grad_norm': 11.395428657531738, 'learning_rate': 2.595e-05, 'epoch': 1.54}\n",
      "{'loss': 0.9122, 'grad_norm': 30.428863525390625, 'learning_rate': 2.58e-05, 'epoch': 1.56}\n",
      "{'loss': 0.9246, 'grad_norm': 7.882857322692871, 'learning_rate': 2.565e-05, 'epoch': 1.58}\n",
      "{'loss': 0.7661, 'grad_norm': 8.264251708984375, 'learning_rate': 2.55e-05, 'epoch': 1.6}\n",
      "{'loss': 0.8372, 'grad_norm': 5.214565753936768, 'learning_rate': 2.535e-05, 'epoch': 1.62}\n",
      "{'loss': 0.8626, 'grad_norm': 6.758748531341553, 'learning_rate': 2.52e-05, 'epoch': 1.64}\n",
      "{'loss': 0.8529, 'grad_norm': 5.057679176330566, 'learning_rate': 2.505e-05, 'epoch': 1.66}\n",
      "{'loss': 0.7982, 'grad_norm': 17.593299865722656, 'learning_rate': 2.49e-05, 'epoch': 1.68}\n",
      "{'loss': 0.8316, 'grad_norm': 13.20866584777832, 'learning_rate': 2.475e-05, 'epoch': 1.7}\n",
      "{'loss': 0.8389, 'grad_norm': 10.265387535095215, 'learning_rate': 2.4599999999999998e-05, 'epoch': 1.72}\n",
      "{'loss': 0.7013, 'grad_norm': 1.5432511568069458, 'learning_rate': 2.4449999999999998e-05, 'epoch': 1.74}\n",
      "{'loss': 0.9006, 'grad_norm': 15.968379974365234, 'learning_rate': 2.43e-05, 'epoch': 1.76}\n",
      "{'loss': 0.9196, 'grad_norm': 4.884054660797119, 'learning_rate': 2.415e-05, 'epoch': 1.78}\n",
      "{'loss': 0.7376, 'grad_norm': 11.74022388458252, 'learning_rate': 2.4e-05, 'epoch': 1.8}\n",
      "{'loss': 0.9855, 'grad_norm': 9.142364501953125, 'learning_rate': 2.385e-05, 'epoch': 1.82}\n",
      "{'loss': 0.7973, 'grad_norm': 4.432516574859619, 'learning_rate': 2.37e-05, 'epoch': 1.84}\n",
      "{'loss': 0.6793, 'grad_norm': 7.232935905456543, 'learning_rate': 2.3550000000000003e-05, 'epoch': 1.86}\n",
      "{'loss': 0.6832, 'grad_norm': 7.303028106689453, 'learning_rate': 2.3400000000000003e-05, 'epoch': 1.88}\n",
      "{'loss': 0.9603, 'grad_norm': 6.29385232925415, 'learning_rate': 2.3250000000000003e-05, 'epoch': 1.9}\n",
      "{'loss': 0.7186, 'grad_norm': 10.51258659362793, 'learning_rate': 2.3100000000000002e-05, 'epoch': 1.92}\n",
      "{'loss': 0.8126, 'grad_norm': 9.812285423278809, 'learning_rate': 2.2950000000000002e-05, 'epoch': 1.94}\n",
      "{'loss': 0.9416, 'grad_norm': 5.39103889465332, 'learning_rate': 2.2800000000000002e-05, 'epoch': 1.96}\n",
      "{'loss': 0.7403, 'grad_norm': 7.235740661621094, 'learning_rate': 2.265e-05, 'epoch': 1.98}\n",
      "{'loss': 0.7801, 'grad_norm': 6.434185981750488, 'learning_rate': 2.25e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6161, 'grad_norm': 2.921614408493042, 'learning_rate': 2.235e-05, 'epoch': 2.02}\n",
      "{'loss': 0.8279, 'grad_norm': 2.798022985458374, 'learning_rate': 2.22e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4739, 'grad_norm': 4.423667907714844, 'learning_rate': 2.205e-05, 'epoch': 2.06}\n",
      "{'loss': 0.6947, 'grad_norm': 7.551945209503174, 'learning_rate': 2.19e-05, 'epoch': 2.08}\n",
      "{'loss': 0.7555, 'grad_norm': 3.4474573135375977, 'learning_rate': 2.175e-05, 'epoch': 2.1}\n",
      "{'loss': 0.5823, 'grad_norm': 11.55009937286377, 'learning_rate': 2.16e-05, 'epoch': 2.12}\n",
      "{'loss': 0.6495, 'grad_norm': 1.750853180885315, 'learning_rate': 2.145e-05, 'epoch': 2.14}\n",
      "{'loss': 0.7813, 'grad_norm': 11.683989524841309, 'learning_rate': 2.13e-05, 'epoch': 2.16}\n",
      "{'loss': 0.6725, 'grad_norm': 3.627124786376953, 'learning_rate': 2.115e-05, 'epoch': 2.18}\n",
      "{'loss': 0.6488, 'grad_norm': 12.301414489746094, 'learning_rate': 2.1e-05, 'epoch': 2.2}\n",
      "{'loss': 0.7546, 'grad_norm': 8.819714546203613, 'learning_rate': 2.085e-05, 'epoch': 2.22}\n",
      "{'loss': 0.7009, 'grad_norm': 5.4411187171936035, 'learning_rate': 2.07e-05, 'epoch': 2.24}\n",
      "{'loss': 0.6692, 'grad_norm': 3.2070164680480957, 'learning_rate': 2.055e-05, 'epoch': 2.26}\n",
      "{'loss': 0.7179, 'grad_norm': 8.569138526916504, 'learning_rate': 2.04e-05, 'epoch': 2.28}\n",
      "{'loss': 0.5266, 'grad_norm': 9.237876892089844, 'learning_rate': 2.025e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6429, 'grad_norm': 5.414910793304443, 'learning_rate': 2.01e-05, 'epoch': 2.32}\n",
      "{'loss': 0.7382, 'grad_norm': 6.2046685218811035, 'learning_rate': 1.995e-05, 'epoch': 2.34}\n",
      "{'loss': 0.5437, 'grad_norm': 1.7678418159484863, 'learning_rate': 1.98e-05, 'epoch': 2.36}\n",
      "{'loss': 0.771, 'grad_norm': 8.791553497314453, 'learning_rate': 1.965e-05, 'epoch': 2.38}\n",
      "{'loss': 0.6883, 'grad_norm': 7.976990699768066, 'learning_rate': 1.95e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6322, 'grad_norm': 3.663111686706543, 'learning_rate': 1.935e-05, 'epoch': 2.42}\n",
      "{'loss': 0.4976, 'grad_norm': 11.463051795959473, 'learning_rate': 1.9200000000000003e-05, 'epoch': 2.44}\n",
      "{'loss': 0.6017, 'grad_norm': 8.745962142944336, 'learning_rate': 1.9050000000000002e-05, 'epoch': 2.46}\n",
      "{'loss': 0.7074, 'grad_norm': 3.3377413749694824, 'learning_rate': 1.8900000000000002e-05, 'epoch': 2.48}\n",
      "{'loss': 0.7662, 'grad_norm': 7.308804035186768, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6001, 'grad_norm': 15.971969604492188, 'learning_rate': 1.86e-05, 'epoch': 2.52}\n",
      "{'loss': 0.5823, 'grad_norm': 6.573346138000488, 'learning_rate': 1.845e-05, 'epoch': 2.54}\n",
      "{'loss': 0.5447, 'grad_norm': 4.459846019744873, 'learning_rate': 1.83e-05, 'epoch': 2.56}\n",
      "{'loss': 0.5825, 'grad_norm': 9.367118835449219, 'learning_rate': 1.815e-05, 'epoch': 2.58}\n",
      "{'loss': 0.4737, 'grad_norm': 9.476505279541016, 'learning_rate': 1.8e-05, 'epoch': 2.6}\n",
      "{'loss': 0.687, 'grad_norm': 5.061061859130859, 'learning_rate': 1.785e-05, 'epoch': 2.62}\n",
      "{'loss': 0.602, 'grad_norm': 6.965111255645752, 'learning_rate': 1.77e-05, 'epoch': 2.64}\n",
      "{'loss': 0.6195, 'grad_norm': 2.466888904571533, 'learning_rate': 1.755e-05, 'epoch': 2.66}\n",
      "{'loss': 0.6031, 'grad_norm': 5.421385765075684, 'learning_rate': 1.74e-05, 'epoch': 2.68}\n",
      "{'loss': 0.4194, 'grad_norm': 6.210037708282471, 'learning_rate': 1.725e-05, 'epoch': 2.7}\n",
      "{'loss': 0.4715, 'grad_norm': 7.969966411590576, 'learning_rate': 1.71e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6358, 'grad_norm': 6.377996444702148, 'learning_rate': 1.695e-05, 'epoch': 2.74}\n",
      "{'loss': 0.5365, 'grad_norm': 5.966507911682129, 'learning_rate': 1.6800000000000002e-05, 'epoch': 2.76}\n",
      "{'loss': 0.6901, 'grad_norm': 10.320416450500488, 'learning_rate': 1.665e-05, 'epoch': 2.78}\n",
      "{'loss': 0.6141, 'grad_norm': 3.961366891860962, 'learning_rate': 1.65e-05, 'epoch': 2.8}\n",
      "{'loss': 0.7512, 'grad_norm': 7.7915544509887695, 'learning_rate': 1.635e-05, 'epoch': 2.82}\n",
      "{'loss': 0.6128, 'grad_norm': 2.9412684440612793, 'learning_rate': 1.62e-05, 'epoch': 2.84}\n",
      "{'loss': 0.8681, 'grad_norm': 7.189391136169434, 'learning_rate': 1.605e-05, 'epoch': 2.86}\n",
      "{'loss': 0.6194, 'grad_norm': 10.706269264221191, 'learning_rate': 1.59e-05, 'epoch': 2.88}\n",
      "{'loss': 0.5759, 'grad_norm': 6.6131463050842285, 'learning_rate': 1.575e-05, 'epoch': 2.9}\n",
      "{'loss': 0.5824, 'grad_norm': 7.8169426918029785, 'learning_rate': 1.56e-05, 'epoch': 2.92}\n",
      "{'loss': 0.6359, 'grad_norm': 8.138054847717285, 'learning_rate': 1.545e-05, 'epoch': 2.94}\n",
      "{'loss': 0.6695, 'grad_norm': 4.247283458709717, 'learning_rate': 1.53e-05, 'epoch': 2.96}\n",
      "{'loss': 0.6396, 'grad_norm': 6.022161483764648, 'learning_rate': 1.515e-05, 'epoch': 2.98}\n",
      "{'loss': 0.7018, 'grad_norm': 7.5884175300598145, 'learning_rate': 1.5e-05, 'epoch': 3.0}\n",
      "{'loss': 0.6837, 'grad_norm': 15.921136856079102, 'learning_rate': 1.485e-05, 'epoch': 3.02}\n",
      "{'loss': 0.499, 'grad_norm': 7.206106185913086, 'learning_rate': 1.47e-05, 'epoch': 3.04}\n",
      "{'loss': 0.4772, 'grad_norm': 6.568816184997559, 'learning_rate': 1.455e-05, 'epoch': 3.06}\n",
      "{'loss': 0.3859, 'grad_norm': 5.152622699737549, 'learning_rate': 1.44e-05, 'epoch': 3.08}\n",
      "{'loss': 0.5022, 'grad_norm': 11.141571998596191, 'learning_rate': 1.4249999999999999e-05, 'epoch': 3.1}\n",
      "{'loss': 0.465, 'grad_norm': 4.181894302368164, 'learning_rate': 1.4099999999999999e-05, 'epoch': 3.12}\n",
      "{'loss': 0.3526, 'grad_norm': 5.728801250457764, 'learning_rate': 1.395e-05, 'epoch': 3.14}\n",
      "{'loss': 0.5738, 'grad_norm': 4.782078266143799, 'learning_rate': 1.3800000000000002e-05, 'epoch': 3.16}\n",
      "{'loss': 0.5763, 'grad_norm': 5.391666412353516, 'learning_rate': 1.3650000000000001e-05, 'epoch': 3.18}\n",
      "{'loss': 0.5167, 'grad_norm': 6.201755046844482, 'learning_rate': 1.3500000000000001e-05, 'epoch': 3.2}\n",
      "{'loss': 0.5179, 'grad_norm': 12.684110641479492, 'learning_rate': 1.3350000000000001e-05, 'epoch': 3.22}\n",
      "{'loss': 0.3208, 'grad_norm': 2.372049570083618, 'learning_rate': 1.32e-05, 'epoch': 3.24}\n",
      "{'loss': 0.4772, 'grad_norm': 9.607600212097168, 'learning_rate': 1.305e-05, 'epoch': 3.26}\n",
      "{'loss': 0.5654, 'grad_norm': 7.315664768218994, 'learning_rate': 1.29e-05, 'epoch': 3.28}\n",
      "{'loss': 0.5754, 'grad_norm': 44.513587951660156, 'learning_rate': 1.275e-05, 'epoch': 3.3}\n",
      "{'loss': 0.4459, 'grad_norm': 8.217463493347168, 'learning_rate': 1.26e-05, 'epoch': 3.32}\n",
      "{'loss': 0.4766, 'grad_norm': 7.956448078155518, 'learning_rate': 1.245e-05, 'epoch': 3.34}\n",
      "{'loss': 0.5423, 'grad_norm': 11.359684944152832, 'learning_rate': 1.2299999999999999e-05, 'epoch': 3.36}\n",
      "{'loss': 0.4423, 'grad_norm': 12.799867630004883, 'learning_rate': 1.215e-05, 'epoch': 3.38}\n",
      "{'loss': 0.4482, 'grad_norm': 4.961115837097168, 'learning_rate': 1.2e-05, 'epoch': 3.4}\n",
      "{'loss': 0.4807, 'grad_norm': 12.46738338470459, 'learning_rate': 1.185e-05, 'epoch': 3.42}\n",
      "{'loss': 0.5123, 'grad_norm': 6.47623872756958, 'learning_rate': 1.1700000000000001e-05, 'epoch': 3.44}\n",
      "{'loss': 0.4781, 'grad_norm': 8.099763870239258, 'learning_rate': 1.1550000000000001e-05, 'epoch': 3.46}\n",
      "{'loss': 0.5534, 'grad_norm': 7.358770370483398, 'learning_rate': 1.1400000000000001e-05, 'epoch': 3.48}\n",
      "{'loss': 0.5251, 'grad_norm': 3.858534097671509, 'learning_rate': 1.125e-05, 'epoch': 3.5}\n",
      "{'loss': 0.4044, 'grad_norm': 5.423594951629639, 'learning_rate': 1.11e-05, 'epoch': 3.52}\n",
      "{'loss': 0.5297, 'grad_norm': 6.002445220947266, 'learning_rate': 1.095e-05, 'epoch': 3.54}\n",
      "{'loss': 0.2627, 'grad_norm': 7.179240703582764, 'learning_rate': 1.08e-05, 'epoch': 3.56}\n",
      "{'loss': 0.5277, 'grad_norm': 2.667809247970581, 'learning_rate': 1.065e-05, 'epoch': 3.58}\n",
      "{'loss': 0.3773, 'grad_norm': 9.669805526733398, 'learning_rate': 1.05e-05, 'epoch': 3.6}\n",
      "{'loss': 0.5899, 'grad_norm': 8.67885684967041, 'learning_rate': 1.035e-05, 'epoch': 3.62}\n",
      "{'loss': 0.5026, 'grad_norm': 7.41753625869751, 'learning_rate': 1.02e-05, 'epoch': 3.64}\n",
      "{'loss': 0.4321, 'grad_norm': 6.049047470092773, 'learning_rate': 1.005e-05, 'epoch': 3.66}\n",
      "{'loss': 0.4866, 'grad_norm': 11.571290969848633, 'learning_rate': 9.9e-06, 'epoch': 3.68}\n",
      "{'loss': 0.3464, 'grad_norm': 1.3661969900131226, 'learning_rate': 9.75e-06, 'epoch': 3.7}\n",
      "{'loss': 0.5142, 'grad_norm': 4.589532852172852, 'learning_rate': 9.600000000000001e-06, 'epoch': 3.72}\n",
      "{'loss': 0.34, 'grad_norm': 4.154843330383301, 'learning_rate': 9.450000000000001e-06, 'epoch': 3.74}\n",
      "{'loss': 0.3956, 'grad_norm': 3.957286834716797, 'learning_rate': 9.3e-06, 'epoch': 3.76}\n",
      "{'loss': 0.4013, 'grad_norm': 36.81330490112305, 'learning_rate': 9.15e-06, 'epoch': 3.78}\n",
      "{'loss': 0.6022, 'grad_norm': 17.906707763671875, 'learning_rate': 9e-06, 'epoch': 3.8}\n",
      "{'loss': 0.5571, 'grad_norm': 8.431528091430664, 'learning_rate': 8.85e-06, 'epoch': 3.82}\n",
      "{'loss': 0.5356, 'grad_norm': 4.372996807098389, 'learning_rate': 8.7e-06, 'epoch': 3.84}\n",
      "{'loss': 0.4747, 'grad_norm': 3.9274160861968994, 'learning_rate': 8.55e-06, 'epoch': 3.86}\n",
      "{'loss': 0.4511, 'grad_norm': 10.406391143798828, 'learning_rate': 8.400000000000001e-06, 'epoch': 3.88}\n",
      "{'loss': 0.4393, 'grad_norm': 4.222914218902588, 'learning_rate': 8.25e-06, 'epoch': 3.9}\n",
      "{'loss': 0.4009, 'grad_norm': 3.814737558364868, 'learning_rate': 8.1e-06, 'epoch': 3.92}\n",
      "{'loss': 0.5626, 'grad_norm': 8.343335151672363, 'learning_rate': 7.95e-06, 'epoch': 3.94}\n",
      "{'loss': 0.481, 'grad_norm': 14.5971040725708, 'learning_rate': 7.8e-06, 'epoch': 3.96}\n",
      "{'loss': 0.3463, 'grad_norm': 4.572031497955322, 'learning_rate': 7.65e-06, 'epoch': 3.98}\n",
      "{'loss': 0.4221, 'grad_norm': 6.59283971786499, 'learning_rate': 7.5e-06, 'epoch': 4.0}\n",
      "{'loss': 0.3654, 'grad_norm': 10.04015827178955, 'learning_rate': 7.35e-06, 'epoch': 4.02}\n",
      "{'loss': 0.4845, 'grad_norm': 8.355648040771484, 'learning_rate': 7.2e-06, 'epoch': 4.04}\n",
      "{'loss': 0.3398, 'grad_norm': 3.299151659011841, 'learning_rate': 7.049999999999999e-06, 'epoch': 4.06}\n",
      "{'loss': 0.3285, 'grad_norm': 8.563228607177734, 'learning_rate': 6.900000000000001e-06, 'epoch': 4.08}\n",
      "{'loss': 0.343, 'grad_norm': 13.18050479888916, 'learning_rate': 6.750000000000001e-06, 'epoch': 4.1}\n",
      "{'loss': 0.4544, 'grad_norm': 8.006757736206055, 'learning_rate': 6.6e-06, 'epoch': 4.12}\n",
      "{'loss': 0.2901, 'grad_norm': 0.395535945892334, 'learning_rate': 6.45e-06, 'epoch': 4.14}\n",
      "{'loss': 0.388, 'grad_norm': 6.399910926818848, 'learning_rate': 6.3e-06, 'epoch': 4.16}\n",
      "{'loss': 0.3283, 'grad_norm': 5.977433204650879, 'learning_rate': 6.1499999999999996e-06, 'epoch': 4.18}\n",
      "{'loss': 0.3811, 'grad_norm': 9.030993461608887, 'learning_rate': 6e-06, 'epoch': 4.2}\n",
      "{'loss': 0.3514, 'grad_norm': 2.7091212272644043, 'learning_rate': 5.850000000000001e-06, 'epoch': 4.22}\n",
      "{'loss': 0.3385, 'grad_norm': 5.382251739501953, 'learning_rate': 5.7000000000000005e-06, 'epoch': 4.24}\n",
      "{'loss': 0.319, 'grad_norm': 4.242626667022705, 'learning_rate': 5.55e-06, 'epoch': 4.26}\n",
      "{'loss': 0.327, 'grad_norm': 5.447913646697998, 'learning_rate': 5.4e-06, 'epoch': 4.28}\n",
      "{'loss': 0.2811, 'grad_norm': 4.877147674560547, 'learning_rate': 5.25e-06, 'epoch': 4.3}\n",
      "{'loss': 0.3193, 'grad_norm': 8.229562759399414, 'learning_rate': 5.1e-06, 'epoch': 4.32}\n",
      "{'loss': 0.4455, 'grad_norm': 7.855600833892822, 'learning_rate': 4.95e-06, 'epoch': 4.34}\n",
      "{'loss': 0.3614, 'grad_norm': 4.983613014221191, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.36}\n",
      "{'loss': 0.5161, 'grad_norm': 3.2183609008789062, 'learning_rate': 4.65e-06, 'epoch': 4.38}\n",
      "{'loss': 0.4833, 'grad_norm': 7.269201755523682, 'learning_rate': 4.5e-06, 'epoch': 4.4}\n",
      "{'loss': 0.3764, 'grad_norm': 5.38096284866333, 'learning_rate': 4.35e-06, 'epoch': 4.42}\n",
      "{'loss': 0.3813, 'grad_norm': 4.224660396575928, 'learning_rate': 4.2000000000000004e-06, 'epoch': 4.44}\n",
      "{'loss': 0.3366, 'grad_norm': 3.9027035236358643, 'learning_rate': 4.05e-06, 'epoch': 4.46}\n",
      "{'loss': 0.3877, 'grad_norm': 6.840775012969971, 'learning_rate': 3.9e-06, 'epoch': 4.48}\n",
      "{'loss': 0.3788, 'grad_norm': 4.390126705169678, 'learning_rate': 3.75e-06, 'epoch': 4.5}\n",
      "{'loss': 0.2474, 'grad_norm': 2.234748125076294, 'learning_rate': 3.6e-06, 'epoch': 4.52}\n",
      "{'loss': 0.27, 'grad_norm': 3.3041775226593018, 'learning_rate': 3.4500000000000004e-06, 'epoch': 4.54}\n",
      "{'loss': 0.2608, 'grad_norm': 2.965710163116455, 'learning_rate': 3.3e-06, 'epoch': 4.56}\n",
      "{'loss': 0.274, 'grad_norm': 2.313701629638672, 'learning_rate': 3.15e-06, 'epoch': 4.58}\n",
      "{'loss': 0.3439, 'grad_norm': 2.739022970199585, 'learning_rate': 3e-06, 'epoch': 4.6}\n",
      "{'loss': 0.3933, 'grad_norm': 6.334038734436035, 'learning_rate': 2.8500000000000002e-06, 'epoch': 4.62}\n",
      "{'loss': 0.2508, 'grad_norm': 5.889503002166748, 'learning_rate': 2.7e-06, 'epoch': 4.64}\n",
      "{'loss': 0.3296, 'grad_norm': 2.0849668979644775, 'learning_rate': 2.55e-06, 'epoch': 4.66}\n",
      "{'loss': 0.284, 'grad_norm': 3.813642978668213, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.68}\n",
      "{'loss': 0.4371, 'grad_norm': 5.4762349128723145, 'learning_rate': 2.25e-06, 'epoch': 4.7}\n",
      "{'loss': 0.3592, 'grad_norm': 3.5113823413848877, 'learning_rate': 2.1000000000000002e-06, 'epoch': 4.72}\n",
      "{'loss': 0.4585, 'grad_norm': 2.7931108474731445, 'learning_rate': 1.95e-06, 'epoch': 4.74}\n",
      "{'loss': 0.334, 'grad_norm': 24.732147216796875, 'learning_rate': 1.8e-06, 'epoch': 4.76}\n",
      "{'loss': 0.2215, 'grad_norm': 10.69448471069336, 'learning_rate': 1.65e-06, 'epoch': 4.78}\n",
      "{'loss': 0.3019, 'grad_norm': 9.185750961303711, 'learning_rate': 1.5e-06, 'epoch': 4.8}\n",
      "{'loss': 0.3177, 'grad_norm': 6.25736141204834, 'learning_rate': 1.35e-06, 'epoch': 4.82}\n",
      "{'loss': 0.2949, 'grad_norm': 5.641919136047363, 'learning_rate': 1.2000000000000002e-06, 'epoch': 4.84}\n",
      "{'loss': 0.2798, 'grad_norm': 4.717790126800537, 'learning_rate': 1.0500000000000001e-06, 'epoch': 4.86}\n",
      "{'loss': 0.3835, 'grad_norm': 8.355045318603516, 'learning_rate': 9e-07, 'epoch': 4.88}\n",
      "{'loss': 0.3785, 'grad_norm': 5.235671520233154, 'learning_rate': 7.5e-07, 'epoch': 4.9}\n",
      "{'loss': 0.3551, 'grad_norm': 4.52632999420166, 'learning_rate': 6.000000000000001e-07, 'epoch': 4.92}\n",
      "{'loss': 0.3959, 'grad_norm': 7.671858310699463, 'learning_rate': 4.5e-07, 'epoch': 4.94}\n",
      "{'loss': 0.3423, 'grad_norm': 7.483911514282227, 'learning_rate': 3.0000000000000004e-07, 'epoch': 4.96}\n",
      "{'loss': 0.4859, 'grad_norm': 8.548364639282227, 'learning_rate': 1.5000000000000002e-07, 'epoch': 4.98}\n",
      "{'loss': 0.3393, 'grad_norm': 5.1197733879089355, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 1278.9923, 'train_samples_per_second': 31.275, 'train_steps_per_second': 1.955, 'train_loss': 0.8936518062591553, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9364d669f57a473cbc2c128de86c158c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.7775\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Generate predictions for the test dataset using BERT model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "bert_predictions = predictions.predictions.argmax(-1)\n",
    "accuracy_bert = accuracy_score(y_test, bert_predictions)\n",
    "print(f'BERT Accuracy: {accuracy_bert}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 Explanation in Words:</h3><p>\n",
    "You need to answer the following questions in a markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 How much did you manage to improve performance on the test set? Did you beat \"Zero Hero\" in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "2.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Kaggle Submission</h2><p>\n",
    "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The results should be presented in two columns in csv format: the first column is the data id (0-14999) and the second column includes the predictions for the test set. The first column must be named id and the second column must be named label (otherwise your submission will fail). A sample predication file can be downloaded from Kaggle for each problem. \n",
    "We provide how to save a csv file if you are running Notebook on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = range(15000)\n",
    "# prediction = range(15000)\n",
    "# submission = pd.DataFrame({'id': id, 'label': prediction})\n",
    "# submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "id = range(15000)\n",
    "svm_predictions = svm_model.predict(vectorizer.transform(test_text))\n",
    "nb_predictions = nb_model.predict(vectorizer.transform(test_text))\n",
    "# xgb_predictions = xgb_model.predict(vectorizer.transform(test_text))\n",
    "\n",
    "# Save predictions to CSV\n",
    "svm_submission = pd.DataFrame({'id': id, 'label': svm_predictions})\n",
    "nb_submission = pd.DataFrame({'id': id, 'label': nb_predictions})\n",
    "# xgb_submission = pd.DataFrame({'id': id, 'label': xgb_predictions})\n",
    "\n",
    "svm_submission.to_csv('../submission/svm_predictions.csv', index=False)\n",
    "nb_submission.to_csv('../submission/nb_predictions.csv', index=False)\n",
    "# xgb_submission.to_csv('../submission/xgb_predictions.csv', index=False)\n",
    "# You may use pandas to generate a dataframe with country, date and your predictions first \n",
    "# and then use to_csv to generate a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f52057f53847b38757a965562b804b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Save BERT predictions to CSV\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_dataset = EmotionDataset(test_encodings, [0]*len(test_text))  # Dummy labels\n",
    "\n",
    "# Generate predictions for the test dataset using BERT model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "bert_predictions = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Save BERT predictions to CSV\n",
    "bert_submission = pd.DataFrame({'id': range(len(bert_predictions)), 'label': bert_predictions})\n",
    "bert_submission.to_csv('../submission/bert_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 4: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please cite the papers and open resources you used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
