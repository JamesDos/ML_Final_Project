{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CS 3780/5780 Creative Project: </h2>\n",
    "<h3>Emotion Classification of Natural Language</h3>\n",
    "\n",
    "Names and NetIDs for your group members: James Tu (jt737), Andrew Cheung (aec295)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction:</h3>\n",
    "\n",
    "<p> The creative project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The past programming projects provide templates for how to do this (and you can reuse part of your code if you wish), and the lectures provide some of the methods you can use. So, this creative project brings realism to how you will use machine learning in the real world.  </p>\n",
    "\n",
    "The task you will work on is classifying texts to human emotions. Through words, humans express feelings, articulate thoughts, and communicate our deepest needs and desires. Language helps us interpret the nuances of joy, sadness, anger, and love, allowing us to connect with others on a deeper level. Are you able to train an ML model that recognizes the human emotions expressed in a piece of text? <b>Please read the project description PDF file carefully and follow the instructions there. Also make sure you write your code and answers to all the questions in this Jupyter Notebook </b> </p>\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 0: Basics</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.1 Import:</h3><p>\n",
    "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "    \n",
    "https://pytorch.org/tutorials/\n",
    "    \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.2 Accuracy and Mean Squared Error:</h3><p>\n",
    "To measure your performance in the Kaggle Competition, we are using accuracy. As a recap, accuracy is the percent of labels you predict correctly. To measure this, you can use library functions from sklearn. A simple example is shown below. \n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [3, 2, 1, 0, 1, 2, 3]\n",
    "y_true = [0, 1, 2, 3, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: Basic</h2><p>\n",
    "Note that your code should be commented well and in part 1.4 you can refer to your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1 Load and preprocess the dataset:</h3><p>\n",
    "We provide how to load the data on Kaggle's Notebook.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "# Split into text and label\n",
    "train_text = train[\"text\"]\n",
    "train_label = train[\"label\"]\n",
    "\n",
    "# Load the testing data\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "# Split into text and id\n",
    "test_id = test[\"id\"]\n",
    "test_text = test[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() # Show the first few rows of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head() # Show the first few rows of testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we split our data into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_val = train_test_split(train_text, train_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we vectorize the training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english') # Remove stop words\n",
    "vectorizer_bow = CountVectorizer(stop_words='english') # Remove stop words\n",
    "\n",
    "# Use TF-IDF vectorizer for sentence embeddings\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "# Use bag of words vectorizer for sentence embeddings\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
    "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 Training, Validation and Model Selection:</h3><p>\n",
    "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use grid search to find the best hyperparameters for SVM model\n",
    "params = {\n",
    "  'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "  'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), params, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_C, best_kernel = best_params['C'], best_params['kernel']\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel=best_kernel, C=best_C)\n",
    "svm_model.fit(X_train_tfidf, y_train) # Use TF-IDF embeddings for SVM\n",
    "\n",
    "# Predict and calculate accuracy for SVM model\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use grid search to find the best hyperparameters (alpha for Laplace smoothing) for Naive Bayes\n",
    "params = {'alpha': [0.1 * x for x in range(1, 11)]} # Search alpha values 0.1 to 1\n",
    "grid_search = GridSearchCV(MultinomialNB(), params, scoring='accuracy')\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "best_alpha = grid_search.best_params_['alpha'] # Get the best alpha value\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "nb_model = MultinomialNB(alpha=best_alpha) # Use the best alpha value for Naive Bayes Model\n",
    "nb_model.fit(X_train_bow, y_train) # Use bag of words embeddings for Naive Bayes\n",
    "\n",
    "# Predict and calculate accuracy for Naive Bayes model\n",
    "y_pred_nb = nb_model.predict(X_test_bow)\n",
    "accuracy_nb = accuracy_score(y_val, y_pred_nb) \n",
    "print(f'Naive Bayes Accuracy: {accuracy_nb}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4 Explanation in Words:</h3><p>\n",
    "    You need to answer the following questions in the markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.1 How did you formulate the learning problem?\n",
    "\n",
    "We formulated the learning problem as a text classification problem where we are \n",
    "given an input text and our goal is to predict the emotion of the text. In particular, \n",
    "we are given an input text as a string and our goal is to classify the text into 1 of\n",
    "28 different classes (labeled 0 to 27) representing the semantic emotion of the text. Hence, we formulated\n",
    "this learning problem as a multi-class classification problem.\n",
    "\n",
    "In order to make the text input useful for machine learning, we need to transform the input\n",
    "text into vector representations that can be used by our training algorithms. There are several ways\n",
    "we can vectorize our text, but we decided to use the TF-IDF and Bag of Words vectorization\n",
    "methods, which we will explain why later on when explain the our model selection. \n",
    "The Bag of Words embeddings represent text as vectors of word frequencies, but doesn't consider\n",
    "the order of words. On the other hand, TF-IDF embeddings weight the frequency of words\n",
    "against how often they appear, which highlights more unique words and reduces the weight\n",
    "of commonly used words. To perform these sentence embeddings, we used the sci-kit learn's\n",
    "TfidfVectorizer and CounterVectorizer, which handles the tokenization of the input text and vector output.\n",
    "When tokenizing the text, for both vectorizers, we decided to exclude stop words like \"and\" and \"the\" as they\n",
    "do not contribute much to the emotion of a text and can add noise to our vector representations.\n",
    "\n",
    "1.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "For the two learning methods from class, we decided to use the SVM and Naive Bayes training algorithms. \n",
    "While researching which algorithms to use for this text classification problem, we came across the paper\n",
    "\"Text Categorization with Support Vector Machines\" by Thorsten Joachims. In the paper, Joachims \n",
    "explores how SVMs are particularly effective at text categorization problems. In particular, \n",
    "the paper argues that SVMs are well suited for text categorization since they can \n",
    "handle high-dimensional features spaces well, which is common in text data. Additionally,\n",
    "the paper explain how many text classification problems are linearly seperable in high-dimensional\n",
    "feature space, which would make SVMs more effective by maximizing margins between classes. For these reasons,\n",
    "we decided to use SVM as one of our learning methods. For the vector embeddings used as inputs\n",
    "to the SVM model, we chose to use the TF-IDF embeddings since they provide normalized\n",
    "weights for terms, which would help prevent the decision boundaries from being skewed. \n",
    "\n",
    "For the second learning method, we decided to use Naive Bayes since we felt that it\n",
    "was the most intuitive. The Naive Bayes training algorithm assumes that the \n",
    "features, which in this case are the words, are conditionally independent given the\n",
    "label, which means that the presence or frequency of a word contributes independently \n",
    "to determining the label. Intuitively, text withs words that appear more frequently in a \n",
    "text of a certain label should have a higher probability of being assigned that label.\n",
    "However, we are aware that this assumption of feature independence most likely\n",
    "will not hold for most text since words in text are often correlated, combining to \n",
    "create emotion. While we predict that Naive Bayes would be less effective than SVM due\n",
    "to its relative simplicity, we decided to use Naive Bayes as a baseline for the\n",
    "other text classification methods. For the Naive Bayes model, we used the bag \n",
    "of words embeddings instead of TF-IDF used with the SVM model. \n",
    "This is because Naive Bayes assumes that the features are conditionally\n",
    "independent given the class label, which aligns with the bag of words embeddings,\n",
    "since it only represents the frequency of the terms and doesn't add any additional\n",
    "weights. Hence the freqency of each word in the bag of words embeddings are\n",
    "directly interpreted as a probabilities by the model. \n",
    "\n",
    "1.4.3 How did you do the model selection?\n",
    "\n",
    "For both SVM and Naive Bayes models, we did hyperparameter tuning to find the best hyperparameters.\n",
    "In particular, we used GridSearchCV to find the best hyperparameters for each model based\n",
    "on cross-validated accuracy scores. For the SVM, we tuned two parameters: C and the kernel. \n",
    "For the C value, we tested the values: [0.001, 0.01, 0.1, 1, 10]. We chose\n",
    "these values as they provide a wide range of C values and we limited the highest C\n",
    "to 10 since we didn't want to choose a C that would overfit the training examples\n",
    "too much, as a higher C value caused the model to place more importance on minimizing \n",
    "misclassifications on the training data. For the kernel, we tested the values \n",
    "['linear', 'poly', 'rbf', 'sigmoid'] as these we the kernels types that were\n",
    "specificed on the scikit learn SVM model. Based on our hyperparameter tuning\n",
    "we discovered that C = 1 and kernel = 'linear' achieved the highest accuracy on our validation set, so we \n",
    "selected those values for our SVM model evaluated on the test set. \n",
    "\n",
    "For the Naive Bayes model, we tuned the alpha parameter, which controls the\n",
    "Laplace smoothing, which ensures that there are no zero probabilities for words\n",
    "that are not in the training data. Using scikit learn's GridSearchCV, we tested\n",
    "alpha values from [0.1, 0.2, ..., 1]. We decided to test small alpha values since\n",
    "we wanted our model to be more sensitive to distinguishing features, which\n",
    "we predicted would enable our model to more easily differentiate between\n",
    "different emotions. Based on our results from GridSearchCV, we discovered that\n",
    "alpha = 0.2 was the most effective on our validation set, so we selected that\n",
    "alpha for the Naive Bayes model evaluated on the test set.\n",
    "\n",
    "1.4.4 Does the test performance reach the first baseline \"Tiny Piney\"? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "For the Naive Bayes model, our submission (nb_predictions.csv), our testing accuracy was 0.58750 so it did not reach the first baseline \"Tiney Piney\" (0.65170 accuracy).\n",
    "<!-- ![Naive Bayes Accuracy](../images/NaiveBayes_Accuracy.png) -->\n",
    "\n",
    "For the SVM model (predictions.csv), our testing accuracy was 0.686000 so we did reach the first baseline \"Tiney Piney\" (0.65170 accuracy):\n",
    "![SVM Accuracy](../images/SVM_Accuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Be creative!</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Open-ended Code:</h3><p>\n",
    "You may follow the steps in part 1 again but making innovative changes like using new training algorithms, etc. Make sure you explain everything clearly in part 2.2. Note that beating \"Zero Hero\" is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load BERT tokenizer and model (Commented out because we used BERT initially, but switched to RoBERTa)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(train_label)))\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(set(train_label)))\n",
    "\n",
    "# Stratified K Fold to ensure class balance between folds\n",
    "strat_k_fold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(strat_k_fold.split(train_text, train_label)):\n",
    "\n",
    "    # Split the data\n",
    "    X_train_fold, X_val_fold = np.array(train_text)[train_idx], np.array(train_text)[val_idx]\n",
    "    y_train_fold, y_val_fold = np.array(train_label)[train_idx], np.array(train_label)[val_idx]\n",
    "\n",
    "    # Tokenize the data\n",
    "    train_encodings = tokenizer(X_train_fold.tolist(), truncation=True, padding=True, max_length=128)\n",
    "    val_encodings = tokenizer(X_val_fold.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    class EmotionDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_dataset = EmotionDataset(train_encodings, y_train_fold.tolist())\n",
    "    val_dataset = EmotionDataset(val_encodings, y_val_fold.tolist())\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',  # specify the output directory\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=3e-5,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with EarlyStoppingCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, p.predictions.argmax(-1))},\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = trainer.predict(val_dataset)\n",
    "    preds = np.argmax(y_pred.predictions, axis=1)\n",
    "    accuracy = accuracy_score(y_val_fold, preds)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average Cross-Validation Accuracy: {average_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 Explanation in Words:</h3><p>\n",
    "You need to answer the following questions in a markdown cell after this cell:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 How much did you manage to improve performance on the test set? Did you beat \"Zero Hero\" in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "With our SVM approach, we managed to achieve an accuracy of 0.68600, but with our approach using BERT, we managed to raise the accuracy by 0.0844 to 0.77040. Below is a screenshot of the the kaggle submission accuracy on the leaderboard, our team name is \"Lebron James\":\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "2.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this.\n",
    "\n",
    "We were trying to research transformers and existing models for NLP tasks like this one for classifying emotions and came across BERT (Bidirectional Encoder Representations from Transformers). We found that since BERT was designed to understand the contextual meaning of words by considering both the left and right context in a sentence, it would be relevant, useful, and potentially more accurate than other approaches such as Naive Bayes for this particular task of classifying emotions from text, which is oftentimes very dependent on context. We also came across RoBERTa, which builds on BERT with some differences such as including more training data and optimizing hyperparameters like batch size and learning rate. As such, we decided to experiment with both models, and we ultimately decided to move forward with RoBERTa, but kept the BERT tokenizer and model commented out.\n",
    "\n",
    "Additionally, we wanted to prevent overfitting and generate a more reliable estimate of the model's performance since we were given a smaller training set of 10,000 entries, so we decided to make use of scikit learn's StratifiedKFold for cross-validation with 5 folds. We experimented with certain training parameters as well including the number of epochs, weight decay, and learning rate to avoid overfitting and optimize the model's performance. We also included EarlyStoppingCallback to allows the model to halt training early if the validation accuracy stops improving. Too many epochs could result in the model memorizing the data and too little would result in the model not having enough time to learn for a good performance, so we tried to experiment with the number of epochs around 3-5 since the training set was on the smaller side. We also experimented similarly around 2e-5 to 5e-5 for the learning rate to strike a balance between efficient convergence and avoiding overshooting the minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Kaggle Submission</h2><p>\n",
    "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The results should be presented in two columns in csv format: the first column is the data id (0-14999) and the second column includes the predictions for the test set. The first column must be named id and the second column must be named label (otherwise your submission will fail). A sample predication file can be downloaded from Kaggle for each problem. \n",
    "We provide how to save a csv file if you are running Notebook on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "id = range(15000)\n",
    "# Vectorize the test data and make predictions\n",
    "svm_predictions = svm_model.predict(vectorizer_tfidf.transform(test_text))\n",
    "nb_predictions = nb_model.predict(vectorizer_bow.transform(test_text))\n",
    "\n",
    "# Save predictions to CSV\n",
    "svm_submission = pd.DataFrame({'id': id, 'label': svm_predictions})\n",
    "nb_submission = pd.DataFrame({'id': id, 'label': nb_predictions})\n",
    "\n",
    "svm_submission.to_csv('../submission/svm_predictions.csv', index=False)\n",
    "nb_submission.to_csv('../submission/nb_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_dataset = EmotionDataset(test_encodings, [0]*len(test_text)) \n",
    "\n",
    "# Generate predictions for the test dataset using BERT model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "bert_predictions = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Save predictions to CSV\n",
    "bert_submission = pd.DataFrame({'id': range(len(bert_predictions)), 'label': bert_predictions})\n",
    "bert_submission.to_csv('../submission/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 4: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please cite the papers and open resources you used.\n",
    "\n",
    "Papers:\n",
    "- SVM paper: https://www.cs.cornell.edu/~tj/publications/joachims_98a.pdf\n",
    "\n",
    "scikit-learn documentation used:\n",
    "- TfidfVectorizer: https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "- CountVectorizer: https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- MultinomialNB: https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "- SVC: https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\n",
    "- GridSearchCV: https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- train_test_split: https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- stratified k fold validation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "\n",
    "BERT and RoBERTa resources:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/bert \n",
    "- https://arxiv.org/pdf/1810.04805\n",
    "- https://huggingface.co/docs/transformers/en/index\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/roberta\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_final_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
